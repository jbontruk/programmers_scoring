{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblioteki i directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and dir\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')   \n",
    "os.chdir(r\"C:\\Users\\jaroslaw.bontruk\\Documents\\Repos\\ITM.Internal.AdvancedSourcing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dane z bazy danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from database\n",
    "db_connection_str = 'mysql+pymysql://###/stackoverflow'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "\n",
    "tags = pd.read_sql('SELECT * FROM tags', con=db_connection)\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele pre-trenowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Google 300\n",
    "model_google_300 = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)\n",
    "#model_google_300.word_vec(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Text8\n",
    "corpus_text8 = api.load('text8')\n",
    "model_text8 = Word2Vec(corpus_text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results\n",
    "print(model_google_300.similarity('frog', 'lizard'))\n",
    "print(model_google_300.similarity('javascript', 'java'))\n",
    "print(model_google_300.similarity('javascript', '.net'))\n",
    "print(model_google_300.similarity('javascript', 'python'))\n",
    "\n",
    "print(model_google_300.most_similar('javascript'))\n",
    "print(model_google_300.most_similar('java'))\n",
    "print(model_google_300.most_similar('python'))\n",
    "print(model_google_300.most_similar('php'))\n",
    "print(model_google_300.most_similar('c#'))\n",
    "\n",
    "print(model_text8.most_similar('javascript'))\n",
    "print(model_text8.most_similar('java'))\n",
    "print(model_text8.most_similar('python'))\n",
    "print(model_text8.most_similar('php'))\n",
    "print(model_text8.most_similar('c#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pre-trained models\n",
    "info = api.info()\n",
    "print(json.dumps(info, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening na Post'ach bez czyszczenia Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "I have multiple Xcode projects inside a workspace. Each project consists of static library target and test target. All test targets perform when I select single project.Question: It is possible to create a single project, that runs all the tests in other projects?\n"
     ]
    }
   ],
   "source": [
    "# Get PostsBody\n",
    "so_corpus = pd.read_csv('./data/corpus.csv')\n",
    "so_corpus_list = so_corpus['body'].tolist()\n",
    "print(type(so_corpus_list))\n",
    "print(so_corpus_list[652235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input for own model (without cleaning)\n",
    "so_data = []\n",
    "\n",
    "for i in range(0,len(so_corpus_list)):\n",
    "    k = so_corpus_list[i]\n",
    "    temp = []\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(k): \n",
    "        temp.append(j.lower()) \n",
    "    so_data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train my own model on tokenized data\n",
    "# Create CBOW model \n",
    "model_so1 = gensim.models.Word2Vec(so_data, min_count = 10, size = 100, workers = 4, window = 5)  \n",
    "# Create Skip Gram model \n",
    "model_so2 = gensim.models.Word2Vec(so_data, min_count = 10, size = 100, workers = 4, window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "pickle.dump(model_so1, open('model_so1.model', 'wb'))\n",
    "pickle.dump(model_so2, open('model_so2.model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results MY CBOW MODEL\n",
    "print(model_so1.similarity('javascript', 'java'))\n",
    "print(model_so1.similarity('javascript', '.net'))\n",
    "print(model_so1.similarity('javascript', 'python'))\n",
    "\n",
    "print(model_so1.most_similar('javascript'))\n",
    "print(model_so1.most_similar('java'))\n",
    "print(model_so1.most_similar('python'))\n",
    "print(model_so1.most_similar('php'))\n",
    "print(model_so1.most_similar('c#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results MY SKIP GRAM MODEL\n",
    "print(model_so2.similarity('javascript', 'java'))\n",
    "print(model_so2.similarity('javascript', '.net'))\n",
    "print(model_so2.similarity('javascript', 'python'))\n",
    "\n",
    "print(model_so2.most_similar('javascript'))\n",
    "print(model_so2.most_similar('java'))\n",
    "print(model_so2.most_similar('python'))\n",
    "print(model_so2.most_similar('php'))\n",
    "print(model_so2.most_similar('c#'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening na Post'ach z czyszczeniem Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PostsBody with string conversion (for no errors with Spacy)\n",
    "so_corpus = pd.read_csv('./data/corpus.csv')\n",
    "so_corpus['body'] = so_corpus['body'].astype(str)\n",
    "print(so_corpus[\"body\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the corpus\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_function(text):\n",
    "    text = nlp(text)\n",
    "    attribs = ['orth_', 'lemma_', 'tag_', 'pos_', 'dep_', 'head']\n",
    "    table = [{att:tok.__getattribute__(att) for att in attribs} for tok in text]\n",
    "    df = pd.DataFrame(table)\n",
    "    df['lemma_'] = df['lemma_'].str.lower()\n",
    "    #df = df[~df['lemma_'].isin([\"-pron-\"])]\n",
    "    df = df[df['pos_'].isin([\"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\"])] \n",
    "    df = df[\"lemma_\"].tolist()\n",
    "    df = \" \".join(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "so_corpus[\"New\"] = so_corpus[\"body\"].apply(lambda x: spacy_function(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cleaned corpus\n",
    "so_corpus_list_cleaned = so_corpus['New'].tolist()\n",
    "print(so_corpus_list_cleaned[623456])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned input for own model\n",
    "so_data_cleaned = []\n",
    "\n",
    "for i in range(0,len(so_corpus_list_cleaned)):\n",
    "    k = so_corpus_list_cleaned[i]\n",
    "    temp = []\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(k): \n",
    "        temp.append(j.lower()) \n",
    "    so_data_cleaned.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train my own model on tokenized data\n",
    "# Create CBOW model \n",
    "model_cleaned_so1 = gensim.models.Word2Vec(so_data_cleaned, min_count = 10, size = 100, workers = 4, window = 5)  \n",
    "# Create Skip Gram model \n",
    "model_cleaned_so2 = gensim.models.Word2Vec(so_data_cleaned, min_count = 10, size = 100, workers = 4, window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "pickle.dump(model_cleaned_so1, open('model_cleaned_so1.model', 'wb'))\n",
    "pickle.dump(model_cleaned_so2, open('model_cleaned_so2.model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results MY CBOW MODEL after data cleaning\n",
    "print(model_cleaned_so1.similarity('javascript', 'java'))\n",
    "print(model_cleaned_so1.similarity('javascript', '.net'))\n",
    "print(model_cleaned_so1.similarity('javascript', 'python'))\n",
    "\n",
    "print(model_cleaned_so1.most_similar('javascript'))\n",
    "print(model_cleaned_so1.most_similar('java'))\n",
    "print(model_cleaned_so1.most_similar('python'))\n",
    "print(model_cleaned_so1.most_similar('php'))\n",
    "print(model_cleaned_so1.most_similar('c#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results MY SKIP GRAM MODEL data cleaning\n",
    "print(model_cleaned_so2.similarity('sql', 'mysql'))\n",
    "print(model_cleaned_so2.similarity('javascript', 'java'))\n",
    "print(model_cleaned_so2.similarity('javascript', '.net'))\n",
    "print(model_cleaned_so2.similarity('javascript', 'python'))\n",
    "\n",
    "print(model_cleaned_so2.most_similar('javascript'))\n",
    "print(model_cleaned_so2.most_similar('java'))\n",
    "print(model_cleaned_so2.most_similar('python'))\n",
    "print(model_cleaned_so2.most_similar('php'))\n",
    "print(model_cleaned_so2.most_similar('c#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cleaned_so2.most_similar('r'))\n",
    "print(model_cleaned_so2.most_similar('c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening na łańcuchach tagów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tags Chains\n",
    "tc_corpus = pd.read_csv('./data/tags_chains.csv')\n",
    "tc_corpus['tags']= tc_corpus['tags'].astype(str)\n",
    "print(tc_corpus['tags'].dtype)\n",
    "# Convert corpus to list\n",
    "tc_corpus_list = tc_corpus['tags'].tolist()\n",
    "print(so_corpus_list_cleaned[123456])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned input for own model\n",
    "tc_data = []\n",
    "\n",
    "for i in range(0,len(tc_corpus_list)):\n",
    "    k = tc_corpus_list[i]\n",
    "    temp = []\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(k): \n",
    "        temp.append(j.lower()) \n",
    "    tc_data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train my own model on tokenized data\n",
    "# Create CBOW model \n",
    "model_tc1 = gensim.models.Word2Vec(tc_data, min_count = 10, size = 100, workers = 4, window = 5)  \n",
    "# Create Skip Gram model \n",
    "model_tc2 = gensim.models.Word2Vec(tc_data, min_count = 10, size = 100, workers = 4, window = 5, sg = 1)\n",
    "# Save models\n",
    "pickle.dump(model_tc1, open('model_tc1.model', 'wb'))\n",
    "pickle.dump(model_tc2, open('model_tc2.model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['java', 'c', 'python', 'c++', '.net', 'javascript', 'php', 'swift',\n",
    "         'sql', 'ruby', 'delphi', 'go', 'd', 'r', 'perl', 'matlab']\n",
    "not_found = ['c#', 'visual_basic', 'object_pascal', 'objective-c', 'assembly_language']\n",
    "tags = ['html', 'python', 'r', 'css', 'regex', 'mysql', 'angular', 'django', 'node', 'node.js', 'nodejs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa  = pd.DataFrame(columns = ['group', 'tag', 'similarity_score'])\n",
    "\n",
    "for i in range(1, len(tags)):\n",
    "    for j in range(1, len(groups)):\n",
    "        mapa.loc[len(mapa)] = [groups[j], tags[i], model_cleaned_so2.similarity(tags[i], groups[j])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = mapa.sort_values(by = ['similarity_score'], ascending = False)\n",
    "mapa.to_csv('mapa4.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
